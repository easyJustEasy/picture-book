import torch
from diffusers import FluxPipeline
from modelscope import snapshot_download
from optimum.quanto import quantize, qfloat8, freeze
import warnings
import os
import time
import psutil
import logging
from datetime import datetime
import sys
import GPUtil
import uuid
from functools import wraps
from safetensors.torch import load_file
from transformers import (
    CLIPTextModel,
    CLIPTokenizer,
    T5EncoderModel,
    T5TokenizerFast,
    pipeline,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,ChineseCLIPProcessor, ChineseCLIPTextModel
)
from pathlib import Path

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("flux_quantization.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("FluxQuantization")
current_working_directory = str(Path(__file__).resolve().parent)

batch=2
# å®šä¹‰è§†è§‰å…ƒç´ 
EMOJI = {
    "start": "ğŸš€",
    "system": "ğŸ–¥ï¸",
    "download": "ğŸ“¥",
    "model": "ğŸ§ ",
    "quant": "âš¡",
    "freeze": "â„ï¸",
    "offload": "ğŸ’¾",
    "generate": "ğŸ¨",
    "success": "âœ…",
    "warning": "âš ï¸",
    "error": "âŒ",
    "complete": "ğŸ†",
    "time": "â±ï¸",
    "gpu": "ğŸ®",
    "cpu": "ğŸ’»",
    "memory": "ğŸ§®",
    "image": "ğŸ–¼ï¸",
    "decorator": "â³",
    "chinese": "ğŸ‡¨ğŸ‡³"

}



# ==================== è®¡æ—¶è£…é¥°å™¨ ====================
def timeit(description=None, emoji=None):
    """å¸¦è¡¨æƒ…ç¬¦å·çš„è®¡æ—¶è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # ç”Ÿæˆé»˜è®¤æè¿°å’Œè¡¨æƒ…
            desc = description or func.__name__.replace('_', ' ').title()
            emoji_char = emoji or EMOJI['decorator']
            
            # å¼€å§‹è®¡æ—¶
            start_time = time.time()
            logger.info(f"{emoji_char} å¼€å§‹: {desc}...")
            
            # æ‰§è¡Œå‡½æ•°
            result = func(*args, **kwargs)
            
            # è®¡ç®—è€—æ—¶
            elapsed = time.time() - start_time
            logger.info(f"{EMOJI['success']} å®Œæˆ: {desc}!")
            logger.info(f"  {EMOJI['time']} è€—æ—¶: {elapsed:.2f}ç§’")
            
            return result
        return wrapper
    return decorator

# ==================== ç³»ç»Ÿä¿¡æ¯ ====================
@timeit("ç³»ç»Ÿä¿¡æ¯æ”¶é›†", EMOJI['system'])
def print_system_info():
    """æ‰“å°è¯¦ç»†çš„ç³»ç»Ÿä¿¡æ¯"""
    logger.info(f"{EMOJI['system']} ===== ç³»ç»Ÿè¯¦æƒ… =====")
    logger.info(f"  {EMOJI['time']} å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"  {EMOJI['system']} æ“ä½œç³»ç»Ÿ: {os.name} {os.uname().version}")
    logger.info(f"  {EMOJI['system']} Pythonç‰ˆæœ¬: {sys.version.split()[0]}")
    logger.info(f"  {EMOJI['system']} PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    if torch.cuda.is_available():
        logger.info(f"  {EMOJI['gpu']} CUDAç‰ˆæœ¬: {torch.version.cuda}")
        logger.info(f"  {EMOJI['gpu']} GPUå‹å·: {torch.cuda.get_device_name(0)}")
        logger.info(f"  {EMOJI['memory']} GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory/1024**3:.2f} GB")
    else:
        logger.info(f"  {EMOJI['warning']} CUDAä¸å¯ç”¨ - ä½¿ç”¨CPUæ¨¡å¼")
    
    logger.info(f"  {EMOJI['cpu']} CPUæ ¸å¿ƒæ•°: {psutil.cpu_count(logical=False)} (é€»è¾‘æ ¸å¿ƒ: {psutil.cpu_count()})")
    logger.info(f"  {EMOJI['memory']} æ€»å†…å­˜: {psutil.virtual_memory().total/1024**3:.2f} GB")
    logger.info(f"{EMOJI['system']} ====================")

# ==================== æ¨¡å‹ä¿¡æ¯æ‰“å° ====================
def print_model_info(model, name):
    """æ‰“å°æ¨¡å‹ä¿¡æ¯"""
    logger.info(f"\n{EMOJI['model']} ===== {name} æ¨¡å‹æ¶æ„ =====")
    logger.info(f"  {EMOJI['model']} ç±»å‹: {type(model).__name__}")
    logger.info(f"  {EMOJI['memory']} å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    logger.info(f"  {EMOJI['cpu']} è®¾å¤‡: {next(model.parameters()).device}")
    logger.info(f"  {EMOJI['model']} æ•°æ®ç±»å‹: {next(model.parameters()).dtype}")
    logger.info(f"{EMOJI['model']} =========================")

# ==================== GPUå†…å­˜ç›‘æ§ ====================
@timeit("GPUå†…å­˜å¿«ç…§", EMOJI['gpu'])
def log_gpu_memory(context=""):
    """é«˜çº§GPUå†…å­˜ç›‘æ§"""
    if context:
        logger.info(f"{EMOJI['memory']} {context}èµ„æºçŠ¶æ€:")
    
    if torch.cuda.is_available():
        mem_alloc = torch.cuda.memory_allocated() / 1024**3
        mem_reserved = torch.cuda.memory_reserved() / 1024**3
        
        # è·å–æ›´è¯¦ç»†çš„GPUä¿¡æ¯
        gpus = GPUtil.getGPUs()
        if gpus:
            gpu = gpus[0]
            logger.info(f"{EMOJI['gpu']} GPUå†…å­˜: {gpu.memoryUsed:.2f}/{gpu.memoryTotal:.2f} GB (ä½¿ç”¨ç‡: {gpu.memoryUtil*100:.1f}%)")
            logger.info(f"{EMOJI['gpu']} GPUè´Ÿè½½: è®¡ç®— {gpu.load*100:.1f}% | æ˜¾å­˜ {gpu.memoryUtil*100:.1f}% | æ¸©åº¦ {gpu.temperature}Â°C")
        else:
            logger.info(f"{EMOJI['gpu']} GPUå†…å­˜: å·²åˆ†é… {mem_alloc:.2f} GB | å·²ä¿ç•™ {mem_reserved:.2f} GB")
    
    # æ·»åŠ CPUå†…å­˜ä¿¡æ¯
    cpu_mem = psutil.virtual_memory()
    logger.info(f"{EMOJI['cpu']} CPUå†…å­˜: ä½¿ç”¨ {cpu_mem.used/1024**3:.2f}/{cpu_mem.total/1024**3:.2f} GB ({cpu_mem.percent}%)")

# ==================== ä¸»ç¨‹åº ====================

"""ä¸»é‡å­åŒ–åˆ›ä½œæµç¨‹"""
# æ‰“å°å¯åŠ¨æ¨ªå¹…
logger.info(f"\n{EMOJI['start']}{'='*80}")
logger.info(f"{'='*80}")
logger.info(f"{EMOJI['start']} å¯åŠ¨ FLUX-1 é‡å­åŒ–æ¨ç†å¼•æ“")
logger.info(f"{EMOJI['start']} ä»»åŠ¡: ä½¿ç”¨ QFloat8 é‡åŒ–ç”Ÿæˆé«˜æ¸…å›¾åƒ")
logger.info(f"{'='*80}\n")

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ['TORCH_CUDA_ARCH_LIST'] = "8.6"
torch.set_default_device("cpu")  # æ‰€æœ‰æ¨¡å‹é»˜è®¤åŠ è½½åˆ° CPU

# å¿½ç•¥quantoçš„ç‰¹å®šè­¦å‘Š
warnings.filterwarnings("ignore", message="Some weights of .* were not initialized from the model checkpoint.*")
# ä¸‹è½½æ¨¡å‹
@timeit("ä¸‹è½½FLUX-1æ¨¡å‹", EMOJI['download'])
def download_model():
    logger.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½ FLUX æ¨¡å‹...")
    repo = snapshot_download("black-forest-labs/FLUX.1-dev", cache_dir="/mnt/f/models")
    logger.info(f"{EMOJI['model']} å­˜å‚¨ä½ç½®: {repo}")
    logger.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½ LoRA æ¨¡å‹...")
    lora_model_id = snapshot_download("yiwanji/FLUX_xiao_hong_shu_ji_zhi_zhen_shi_V2")
    logger.info(f"{EMOJI['model']} å­˜å‚¨ä½ç½®: {lora_model_id}")
    clip_model_id = snapshot_download("AI-ModelScope/clip-vit-large-patch14")
    return {
        'bfl_repo': repo,
        'lora_model_path': lora_model_id,
        "clip_model_id":clip_model_id
    }


# åˆå§‹åŒ–ç®¡é“
@timeit("åˆå§‹åŒ–é‡å­åŒ–ç®¡é“", EMOJI['model'])
def initialize_pipeline():
    pipe = FluxPipeline.from_pretrained(bfl_repo, 
                                        torch_dtype=torch.bfloat16)
    return pipe

@timeit("è®¾ç½®toker", EMOJI['model'])
def reset_token(pipe):
    # text_encoder = CLIPTextModel.from_pretrained(clip_model_id, torch_dtype=torch.bfloat16,use_fast=False)
    # tokenizer = CLIPTokenizer.from_pretrained(clip_model_id, torch_dtype=torch.bfloat16,use_fast=False)
    # pipe.text_encoder= text_encoder
    # pipe.tokenizer=tokenizer
    print("+++++++++++++++++++==")
# æ³¨å…¥laro
@timeit("æ³¨å…¥ LoRA æƒé‡", EMOJI['quant'])
def inject_lora(pipe):
    # æ³¨å…¥ LoRA æƒé‡
    logger.info("ğŸ“¥ åŠ è½½ LoRA æƒé‡...")
    lora_state_dict = load_file(f'{lora_model_path}/è‹-FLUXæŠ–éŸ³å°çº¢ä¹¦æè‡´çœŸå®_è‹-FLUXå°çº¢ä¹¦æè‡´çœŸå®V2.safetensors')
    pipe.transformer.load_state_dict(lora_state_dict, strict=False)
    

# é‡å­åŒ–æ¨¡å‹
@timeit("é‡å­åŒ–æ¨¡å‹ç»„ä»¶", EMOJI['quant'])
def quantize_model_components(pipe):
    logger.info(f"{EMOJI['quant']} åº”ç”¨ QFloat8 é‡åŒ–åˆ°æ–‡æœ¬ç¼–ç å™¨...")
    quantize(pipe.text_encoder_2, weights=qfloat8)
    logger.info(f"{EMOJI['quant']} åº”ç”¨ QFloat8 é‡åŒ–åˆ° Transformer æ ¸å¿ƒ...")
    quantize(pipe.transformer, weights=qfloat8)
    logger.info(f"{EMOJI['quant']} æ–‡æœ¬ç¼–ç å™¨é‡å­åŒ–çŠ¶æ€")
    logger.info(f"{EMOJI['quant']} Transformeré‡å­åŒ–çŠ¶æ€")

# å†»ç»“æ¨¡å‹
@timeit("å†»ç»“é‡å­åŒ–å‚æ•°", EMOJI['freeze'])
def freeze_model(pipe):
    freeze(pipe.text_encoder_2)
    freeze(pipe.transformer)



# å¯ç”¨CPU offload
@timeit("æ¿€æ´»æ™ºèƒ½å†…å­˜ç®¡ç†", EMOJI['offload'])
def enable_memory_management(pipe):
    pipe.enable_model_cpu_offload()
    
# æ‰“å°ç³»ç»Ÿä¿¡æ¯
print_system_info()   
model_paths = download_model()
bfl_repo = model_paths['bfl_repo']
lora_model_path = model_paths['lora_model_path']
clip_model_id=model_paths["clip_model_id"]    
pipe = initialize_pipeline()
reset_token(pipe)
inject_lora(pipe)
quantize_model_components(pipe)
freeze_model(pipe)
# æ‰“å°æ€»è€—æ—¶
logger.info(f"\n{EMOJI['complete']}{'='*80}")
logger.info(f"{EMOJI['complete']} FLUX-1 é‡å­åŒ–åˆ›ä½œä»»åŠ¡å®Œæˆ!")
logger.info(f"{EMOJI['complete']}{'='*80}")
enable_memory_management(pipe)    
# ç”Ÿæˆå›¾åƒ
@timeit("é‡å­è‰ºæœ¯ç”Ÿæˆ", EMOJI['generate'])
def generate(prompt,step=50,
                            width=1024,
                            height=1024,
                            guidance_scale=3.5,
                            max_sequence_length=512,
                            ):    
    # ç”Ÿæˆå‚æ•°æ˜¾ç¤º
    logger.info(f"{EMOJI['generate']} ç”Ÿæˆå‚æ•°é…ç½®:")
    logger.info(f"  {EMOJI['image']} åˆ†è¾¨ç‡: {width}X{height} (UHD)")
    logger.info(f"  {EMOJI['model']} å¼•å¯¼ç³»æ•°: {guidance_scale}")
    logger.info(f"  {EMOJI['time']} æ¨ç†æ­¥æ•°: {step}")
    logger.info(f"  {EMOJI['model']} æœ€å¤§åºåˆ—é•¿åº¦: {max_sequence_length}")

    # ç”Ÿæˆå‰èµ„æºå¿«ç…§
    log_gpu_memory("ç”Ÿæˆå‰")
    # æ‰§è¡Œç”Ÿæˆ
    logger.info(f"{EMOJI['generate']} å¼€å§‹ç”Ÿæˆé‡å­è‰ºæœ¯...")
    seed = -1
    image = pipe(
        prompt=prompt,
        height=height,
        width=width,
        guidance_scale=guidance_scale,
        num_inference_steps=step,
        max_sequence_length=max_sequence_length,
        generator=torch.Generator(device="cpu").manual_seed(int(seed) if seed != -1 else torch.seed()),
    ).images[0]
    
    # ç”Ÿæˆåèµ„æºå¿«ç…§
    log_gpu_memory("ç”Ÿæˆå")
    return image







# æ‰§è¡Œä¸»ç¨‹åº
if __name__ == "__main__":
    prompt = """
    ä½ å¥½ï¼Œå¯ä»¥ç”Ÿæˆä¸€ä¸ªçŒ«å—ï¼Œç™½è‰²çš„çŒ«
"""
    logger.info(f"{EMOJI['generate']} æç¤ºè¯: '{prompt}'")
    for num in range(0,2):
        image =  generate(prompt,)
        # ä¿å­˜å›¾åƒ
        logger.info(f"{EMOJI['image']} ä¿å­˜é‡å­è‰ºæœ¯ä½œå“...")
        out_put_path = f"{current_working_directory}/temp/flux_{uuid.uuid1()}.png"
        image.save(out_put_path)
        logger.info(f"{EMOJI['success']} ä½œå“å·²ä¿å­˜ä¸º: {out_put_path}")
    # æ‰“å°å³°å€¼å†…å­˜å’Œæ€»è€—æ—¶ï¼ˆè£…é¥°å™¨æ— æ³•æ•è·è¿™äº›ï¼‰
    peak_mem = psutil.Process().memory_info().rss/1024**3
    logger.info(f"  {EMOJI['memory']} å³°å€¼å†…å­˜ä½¿ç”¨: {peak_mem:.2f} GB")
    logger.info(f"{EMOJI['complete']}{'='*80}")
