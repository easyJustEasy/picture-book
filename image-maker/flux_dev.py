import os
import time
import torch
from datetime import datetime
from safetensors.torch import load_file
from optimum.quanto import quantize, qfloat8, freeze
from diffusers import FluxPipeline, FlowMatchEulerDiscreteScheduler, AutoencoderKL
from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel
from transformers import (
    CLIPTextModel,
    CLIPTokenizer,
    T5EncoderModel,
    T5TokenizerFast,
)
from modelscope import snapshot_download
from translator import QwenTranslator
from fp8checker import check_fp8_support, print_support_report
import json
import logging

# é…ç½®æ—¥å¿—ç³»ç»Ÿ
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[logging.StreamHandler()]
)

logger = logging.getLogger("FluxPipeline")

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["TORCH_CUDA_ARCH_LIST"] = "8.6"

torch.set_default_device("cpu")  # æ‰€æœ‰æ¨¡å‹é»˜è®¤åŠ è½½åˆ° CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dtype = torch.bfloat16

# æ£€æŸ¥ FP8 æ”¯æŒ
checker = check_fp8_support()
print_support_report(checker)
HARDWARE_FP8_SUPPORTED = checker['fp8_supported']
logger.info(f"ç¡¬ä»¶FP8æ”¯æŒ: {'âœ…' if HARDWARE_FP8_SUPPORTED else 'âŒ'}")

def print_gpu_memory(step):
    """æ‰“å°å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(device=0) / 1024 ** 3
        cached = torch.cuda.memory_reserved(device=0) / 1024 ** 3
        formatted_now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        logger.info(f"{formatted_now} {step} ===> å·²åˆ†é…: {allocated:.2f} GB | ä¿ç•™ç¼“å­˜: {cached:.2f} GB")

def measure_time(func):
    """è£…é¥°å™¨ï¼šç”¨äºæµ‹é‡å‡½æ•°æ‰§è¡Œæ—¶é—´"""
    def wrapper(*args, **kwargs):
        start_time = time.time()
        logger.info(f"[å¼€å§‹] æ­£åœ¨æ‰§è¡Œ {func.__name__}")
        result = func(*args, **kwargs)
        end_time = time.time()
        logger.info(f"[å®Œæˆ] {func.__name__} è€—æ—¶ {end_time - start_time:.2f} ç§’")
        return result
    return wrapper

@measure_time
def load_translator():
    logger.info("ğŸ§  åŠ è½½ç¿»è¯‘æ¨¡å‹...")
    translator = QwenTranslator()
    logger.info("ğŸ§  ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæ¯•")
    return translator

@measure_time
def download_models():
    logger.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½åŸºç¡€æ¨¡å‹ (black-forest-labs/FLUX.1-dev)...")
    bfl_repo = snapshot_download("black-forest-labs/FLUX.1-dev", cache_dir="/mnt/f/models")

    logger.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½ FP8 æ¨¡å‹...")
    fp8_model_id = snapshot_download("livehouse/flux1-dev-fp8")

    logger.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½ LoRA æ¨¡å‹...")
    lora_model_id = snapshot_download("yiwanji/FLUX_xiao_hong_shu_ji_zhi_zhen_shi_V2")

    return {
        'bfl_repo': bfl_repo,
        'fp8_model_path': f"{fp8_model_id}/flux1-dev-fp8.safetensors",
        'lora_model_path': lora_model_id
    }

@measure_time
def init_scheduler_and_tokenizers(bfl_repo):
    logger.info("ğŸ§  åˆå§‹åŒ– scheduler å’Œ tokenizer...")
    scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder="scheduler")
    tokenizer = CLIPTokenizer.from_pretrained(bfl_repo, subfolder="tokenizer")
    tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder="tokenizer_2")
    return scheduler, tokenizer, tokenizer_2

@measure_time
def init_text_encoders(bfl_repo, dtype):
    logger.info("ğŸ§  åˆå§‹åŒ– text_encoder (CLIP)...")
    text_encoder = CLIPTextModel.from_pretrained(bfl_repo, subfolder="text_encoder", torch_dtype=dtype)

    logger.info("ğŸ§  åˆå§‹åŒ– text_encoder_2 (T5) å¹¶é‡åŒ–å†»ç»“...")
    text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder="text_encoder_2", torch_dtype=dtype)
    
    # å¯¹æ–‡æœ¬ç¼–ç å™¨è¿›è¡Œé‡åŒ–ä»¥èŠ‚çœå†…å­˜
    if not HARDWARE_FP8_SUPPORTED:
        logger.info("ğŸ”§ text_encoder_2 ä½¿ç”¨ qfloat8 é‡åŒ–æ–‡æœ¬ç¼–ç å™¨...")
        quantize(text_encoder_2, weights=qfloat8)
        freeze(text_encoder_2)
    
    return text_encoder, text_encoder_2

@measure_time
def init_vae(bfl_repo, dtype):
    logger.info("ğŸ§  åˆå§‹åŒ– vae...")
    vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder="vae", torch_dtype=dtype)
    return vae

def patch_flux_transformer_forward():
    """åŠ¨æ€ä¿®è¡¥ FluxTransformerBlock çš„ forward æ–¹æ³•ä»¥è§£å†³ attn_output é”™è¯¯"""
    logger.info("ğŸ”§ åº”ç”¨ FluxTransformerBlock è¿è¡Œæ—¶è¡¥ä¸...")
    
    try:
        from diffusers.models.transformers.transformer_flux import FluxTransformerBlock
        
        # ä¿å­˜åŸå§‹ forward æ–¹æ³•
        original_forward = FluxTransformerBlock.forward
        
        # åˆ›å»ºç®€åŒ–çš„ä¿®è¡¥åçš„ forward æ–¹æ³•
        def patched_forward(self, hidden_states, encoder_hidden_states=None, temb=None, attention_mask=None, **kwargs):
            """
            ç®€åŒ–çš„ä¿®è¡¥ forward æ–¹æ³•
            - é¿å…ä½¿ç”¨ä¸å­˜åœ¨çš„å±æ€§
            - åªæ‰§è¡ŒåŸºæœ¬çš„æ“ä½œ
            """
            # è°ƒç”¨ norm1 å±‚
            try:
                # å°è¯•ä½¿ç”¨ temb å‚æ•°
                if temb is not None:
                    norm_hidden_states = self.norm1(hidden_states, temb)
                else:
                    norm_hidden_states = self.norm1(hidden_states)
            except TypeError:
                # å¦‚æœå¤±è´¥ï¼Œåªä¼ å…¥ hidden_states
                norm_hidden_states = self.norm1(hidden_states)
            
            # è°ƒç”¨æ³¨æ„åŠ›å±‚
            attn_output = self.attn1(
                norm_hidden_states,
                encoder_hidden_states=encoder_hidden_states,
                attention_mask=attention_mask,
                **kwargs
            )
            
            # æ®‹å·®è¿æ¥
            hidden_states = attn_output + hidden_states
            
            # è¿”å›ç»“æœ
            return hidden_states
        
        # åº”ç”¨è¡¥ä¸
        FluxTransformerBlock.forward = patched_forward
        logger.info("âœ… FluxTransformerBlock.forward ä¿®è¡¥å®Œæˆ (ç®€åŒ–ç‰ˆ)")
        
    except Exception as e:
        logger.error(f"åº”ç”¨è¡¥ä¸å¤±è´¥: {e}")

@measure_time
def init_transformer(bfl_repo, fp8_model_path, lora_model_path, dtype):
    # åº”ç”¨è¿è¡Œæ—¶è¡¥ä¸
    # patch_flux_transformer_forward()
    
    logger.info("ğŸ§  åˆå§‹åŒ– transformer...")
    transformer = FluxTransformer2DModel.from_pretrained(
        bfl_repo, 
        subfolder="transformer", 
        torch_dtype=dtype
    )
    
    # åŠ è½½ FP8 æƒé‡ï¼ˆå¦‚æœæ”¯æŒï¼‰
    if HARDWARE_FP8_SUPPORTED:
        logger.info("ğŸ”§ åŠ è½½ FP8 é‡åŒ–æƒé‡...")
        state_dict = load_file(fp8_model_path)
        new_state_dict = {}
        for key, value in state_dict.items():
            new_key = key.replace("transformer.", "")
            new_state_dict[new_key] = value
        
        missing, unexpected = transformer.load_state_dict(new_state_dict, strict=False)
        if missing:
            logger.warning(f"ç¼ºå¤±çš„é”®: {missing}")
        if unexpected:
            logger.warning(f"æ„å¤–çš„é”®: {unexpected}")
    
    # æ³¨å…¥ LoRA æƒé‡
    # logger.info("ğŸ“¥ åŠ è½½ LoRA æƒé‡...")
    # lora_state_dict = load_file(f'{lora_model_path}/è‹-FLUXæŠ–éŸ³å°çº¢ä¹¦æè‡´çœŸå®_è‹-FLUXå°çº¢ä¹¦æè‡´çœŸå®V2.safetensors')
    # transformer.load_state_dict(lora_state_dict, strict=False)
    
    # åº”ç”¨é‡åŒ–
    if HARDWARE_FP8_SUPPORTED:
        logger.info("âœ… transformer ä½¿ç”¨ç¡¬ä»¶åŸç”Ÿ FP8")
        transformer = transformer.to(torch.float8_e4m3fn)
    else:
        logger.info("ğŸ”§ transformer ä½¿ç”¨ qfloat8 æ¨¡æ‹Ÿ FP8 é‡åŒ–...")
        quantize(transformer, weights=qfloat8)
        freeze(transformer)
        logger.info("ğŸ”’ æ¨¡å‹å·²é‡åŒ–ä¸º qfloat8 å¹¶å†»ç»“")
    
    return transformer

@measure_time
def build_pipeline(**kwargs):
    logger.info("ğŸ§  æ„å»º FluxPipeline...")
    pipe = FluxPipeline(**kwargs)
    
    # ä½¿ç”¨åŸå§‹è°ƒåº¦å™¨ï¼ˆé¿å…å…¼å®¹æ€§é—®é¢˜ï¼‰
    logger.info("ğŸ”§ ä½¿ç”¨åŸå§‹ FlowMatchEulerDiscreteScheduler è°ƒåº¦å™¨...")
    
    # å¯ç”¨å†…å­˜ä¼˜åŒ–
    pipe.enable_model_cpu_offload()
    pipe.enable_vae_slicing()
    
    # ç¦ç”¨å¯èƒ½å¼•èµ·é—®é¢˜çš„ç‰¹æ€§
    try:
        if hasattr(pipe, 'disable_xformers_memory_efficient_attention'):
            pipe.disable_xformers_memory_efficient_attention()
        if hasattr(pipe, 'disable_freeu'):
            pipe.disable_freeu()
    except Exception as e:
        logger.warning(f"æ— æ³•ç¦ç”¨æŸäº›ç‰¹æ€§: {e}")
    
    return pipe

def translate_text(translator, input_text):
    """ç¿»è¯‘æ–‡æœ¬"""
    try:
        result = translator.translate(input_text)
        return result
    except Exception as e:
        logger.error(f"ç¿»è¯‘å¤±è´¥: {e}")
        return input_text

@measure_time
def init_pipe():
    logger.info("ğŸ“¥ æ­£åœ¨ä¸‹è½½åŸºç¡€æ¨¡å‹...")
    model_paths = download_models()

    logger.info("ğŸ§  åˆå§‹åŒ– scheduler å’Œ tokenizer...")
    scheduler, tokenizer, tokenizer_2 = init_scheduler_and_tokenizers(model_paths['bfl_repo'])

    logger.info("ğŸ§  åˆå§‹åŒ– text_encoder (CLIP)...")
    text_encoder, text_encoder_2 = init_text_encoders(model_paths['bfl_repo'], dtype)

    logger.info("ğŸ§  åˆå§‹åŒ– vae...")
    vae = init_vae(model_paths['bfl_repo'], dtype)

    logger.info("ğŸ§  åˆå§‹åŒ– transformer ...")
    transformer = init_transformer(
        model_paths['bfl_repo'],
        model_paths['fp8_model_path'],
        model_paths['lora_model_path'],
        dtype
    )

    logger.info("ğŸ§  æ„å»º FluxPipeline å¹¶å¯ç”¨è‡ªåŠ¨å¸è½½...")
    pipe = build_pipeline(
        scheduler=scheduler,
        text_encoder=text_encoder,
        tokenizer=tokenizer,
        text_encoder_2=text_encoder_2,
        tokenizer_2=tokenizer_2,
        vae=vae,
        transformer=transformer,
    )

    # æ¸…ç†å†…å­˜
    del text_encoder, text_encoder_2, vae, transformer
    torch.cuda.empty_cache()
    return pipe

def safe_generate_image(pipeline, prompt, steps=20, width=1024, height=1024, guidance=3.5, seed=-1):
    """å®‰å…¨ç”Ÿæˆå›¾åƒï¼ŒåŒ…å«é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶"""
    max_retries = 3
    current_width, current_height = width, height
    
    for attempt in range(max_retries):
        try:
            # åˆ›å»ºç”Ÿæˆå™¨
            if seed == -1:
                generator = torch.Generator(device="cuda").manual_seed(torch.seed())
            else:
                generator = torch.Generator(device="cuda").manual_seed(seed)
            
            # ç”Ÿæˆå›¾åƒ
            image = pipeline(
                prompt=prompt,
                width=current_width,
                height=current_height,
                num_inference_steps=steps,
                guidance_scale=guidance,
                generator=generator,
            ).images[0]
            
            return image
        
        except Exception as e:
            if "attn_output" in str(e) and attempt < max_retries - 1:
                logger.error(f"é‡åˆ° attn_output é”™è¯¯ï¼Œé‡è¯• {attempt+1}/{max_retries}")
                
                # é™ä½åˆ†è¾¨ç‡
                current_width = max(512, int(current_width * 0.8))
                current_height = max(512, int(current_height * 0.8))
                logger.info(f"æ–°åˆ†è¾¨ç‡: {current_width}x{current_height}")
                
                # æ¸…ç†å†…å­˜
                torch.cuda.empty_cache()
            else:
                raise e

@measure_time
def run_pipeline(
    pipeline,
    translator,
    prompt: str,
    steps: int = 50,
    width: int = 1024,
    height: int = 1024,
    guidance: float = 3.5,
    seed: int = -1,
    output_path: str = None
):
    """
    è¿è¡Œå›¾åƒç”Ÿæˆæµç¨‹
    :param pipeline: å›¾åƒç”Ÿæˆç®¡é“
    :param translator: ç¿»è¯‘å™¨
    :param prompt: ç”¨æˆ·è¾“å…¥çš„æç¤ºè¯ï¼ˆä¸­æ–‡ï¼‰
    :param steps: æ¨ç†æ­¥æ•°
    :param width: å›¾åƒå®½åº¦
    :param height: å›¾åƒé«˜åº¦
    :param guidance: å¼•å¯¼ç³»æ•°
    :param seed: éšæœºç§å­ï¼ˆé»˜è®¤ä¸º -1 è¡¨ç¤ºéšæœºï¼‰
    :param output_path: è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤ä¸ºå½“å‰ç›®å½•ä¸‹çš„ test_output.pngï¼‰
    """
    if output_path is None:
        current_working_directory = os.path.dirname(os.path.abspath(__file__))
        output_path = os.path.join(current_working_directory, "test_output.png")

    torch.cuda.empty_cache()
    print_gpu_memory("ç”Ÿæˆå‰æ˜¾å­˜çŠ¶æ€")

    translated_prompt = translate_text(translator, prompt)
    logger.info(f"ğŸ“ Prompt: {prompt} â†’ Translated: {translated_prompt}")

    logger.info("ğŸ§  å¼€å§‹ç”Ÿæˆå›¾ç‰‡...")
    
    # ä½¿ç”¨å®‰å…¨ç”Ÿæˆå‡½æ•°
    image = safe_generate_image(
        pipeline=pipeline,
        prompt=translated_prompt,
        steps=steps,
        width=width,
        height=height,
        guidance=guidance,
        seed=seed
    )

    print_gpu_memory("ç”Ÿæˆåæ˜¾å­˜çŠ¶æ€")
    logger.info(f"ğŸ“¸ å›¾åƒç”Ÿæˆå®Œæˆï¼Œæ­£åœ¨ä¿å­˜è‡³ï¼š{output_path}")
    image.save(output_path)
    return output_path

# ä¸»ç¨‹åº
if __name__ == "__main__":
    try:
        logger.info("ğŸ§  å¼€å§‹åŠ è½½ç¿»è¯‘æ¨¡å‹...")
        translator = load_translator()
        
        logger.info("ğŸš€ åˆå§‹åŒ–å›¾åƒç”Ÿæˆç®¡é“...")
        pipeline = init_pipe()
        
        # æµ‹è¯•ç”Ÿæˆ - ä½¿ç”¨æ›´å°çš„åˆ†è¾¨ç‡è¿›è¡Œåˆå§‹æµ‹è¯•
        generated_image = run_pipeline(
            pipeline=pipeline,
            translator=translator,
            prompt="ä¸€åªå¯çˆ±çš„å°çŒ«",
            steps=20,
            width=768,  # è¾ƒå°åˆ†è¾¨ç‡
            height=768,
            guidance=3.5,
            seed=42  # å›ºå®šç§å­
        )
        
        logger.info(f"âœ… å›¾åƒç”ŸæˆæˆåŠŸ: {generated_image}")
        
        # æˆåŠŸåå°è¯•æ›´é«˜åˆ†è¾¨ç‡
        generated_image = run_pipeline(
            pipeline=pipeline,
            translator=translator,
            prompt="ä¸€åªå¯çˆ±çš„å°çŒ«",
            steps=20,
            width=1280,
            height=720,
            guidance=3.5,
            seed=42
        )
        logger.info(f"âœ… é«˜åˆ†è¾¨ç‡å›¾åƒç”ŸæˆæˆåŠŸ: {generated_image}")
    
    except Exception as e:
        logger.exception("ğŸš¨ ä¸»ç¨‹åºè¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿå¼‚å¸¸")
        
        # æä¾›è¯¦ç»†çš„è¯Šæ–­ä¿¡æ¯
        if "use_ada_layer_norm" in str(e):
            logger.error("æ£€æµ‹åˆ° 'use_ada_layer_norm' å±æ€§é—®é¢˜ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
            logger.error("1. å‡çº§ diffusers åº“åˆ°æœ€æ–°ç‰ˆæœ¬: pip install -U diffusers")
            logger.error("2. æ£€æŸ¥æ¨¡å‹ä¸åº“ç‰ˆæœ¬å…¼å®¹æ€§")
            logger.error("3. ä½¿ç”¨æ›´ç®€å•çš„è¡¥ä¸æ–¹æ³•")
        elif "attn_output" in str(e):
            logger.error("æ£€æµ‹åˆ° attn_output é”™è¯¯ï¼Œè¯·å°è¯•ä»¥ä¸‹è§£å†³æ–¹æ¡ˆï¼š")
            logger.error("1. ä½¿ç”¨æ›´å°çš„åˆ†è¾¨ç‡ï¼ˆå¦‚ 768x768ï¼‰")
            logger.error("2. ä½¿ç”¨ä¸åŒçš„è°ƒåº¦å™¨")
            logger.error("3. æ£€æŸ¥æ¨¡å‹æƒé‡å®Œæ•´æ€§")
        else:
            logger.error(f"æœªçŸ¥é”™è¯¯ç±»å‹: {str(e)}")