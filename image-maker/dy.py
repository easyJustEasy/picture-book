import os
import torch
from datetime import datetime
from safetensors.torch import load_file
from optimum.quanto import quantize, qfloat8, freeze
from diffusers import FluxPipeline, FlowMatchEulerDiscreteScheduler, AutoencoderKL
from diffusers.models.transformers.transformer_flux import FluxTransformer2DModel
from transformers import (
    CLIPTextModel,
    CLIPTokenizer,
    T5EncoderModel,
    T5TokenizerFast,
    pipeline,
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
)
from modelscope import snapshot_download
from translator import QwenTranslator
#from trans import OpusTranslation
# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
os.environ['TORCH_CUDA_ARCH_LIST'] = "8.6"
torch.set_default_device("cpu")  # æ‰€æœ‰æ¨¡å‹é»˜è®¤åŠ è½½åˆ° CPU

# å½“å‰å·¥ä½œç›®å½•
current_working_directory = os.path.dirname(os.path.abspath(__file__))
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
dtype = torch.bfloat16


def print_gpu_memory(step):
    """æ‰“å°å½“å‰ GPU æ˜¾å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        allocated = torch.cuda.memory_allocated(device=0) / 1024 ** 3
        cached = torch.cuda.memory_reserved(device=0) / 1024 ** 3
        formatted_now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        print(f"{formatted_now} {step} ===> å·²åˆ†é…: {allocated:.2f} GB | ä¿ç•™ç¼“å­˜: {cached:.2f} GB")

# åŠ è½½ç¿»è¯‘æ¨¡å‹ï¼ˆé™åˆ¶åœ¨ CPU ä¸Šï¼‰
print("ğŸ§  åŠ è½½ç¿»è¯‘æ¨¡å‹...")
translator = QwenTranslator()
print("ğŸ§  ç¿»è¯‘æ¨¡å‹åŠ è½½å®Œæ¯•...")
# ä¸‹è½½ä¸»æ¨¡å‹å’Œ LoRA æ¨¡å‹
print("ğŸ“¥ æ­£åœ¨ä¸‹è½½åŸºç¡€æ¨¡å‹...")
bfl_repo = snapshot_download("zhusiyuanhao/FLUX1-schnell-fp8")
print("ğŸ“¥ æ­£åœ¨ä¸‹è½½ LoRA æ¨¡å‹...")
lora_model_path = snapshot_download('yiwanji/FLUX_xiao_hong_shu_ji_zhi_zhen_shi_V2')


# åŠ è½½ scheduler å’Œ tokenizer
print("ğŸ§  åˆå§‹åŒ– scheduler å’Œ tokenizer...")
scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(bfl_repo, subfolder="scheduler")
tokenizer = CLIPTokenizer.from_pretrained(bfl_repo, subfolder="tokenizer")
tokenizer_2 = T5TokenizerFast.from_pretrained(bfl_repo, subfolder="tokenizer_2")

# åŠ è½½ text_encoderï¼ˆCLIPï¼‰
print("ğŸ§  åˆå§‹åŒ– text_encoder (CLIP)...")
text_encoder = CLIPTextModel.from_pretrained(bfl_repo, subfolder="text_encoder", torch_dtype=dtype)

# åŠ è½½ text_encoder_2ï¼ˆT5ï¼‰å¹¶è¿›è¡Œ FP8 é‡åŒ– + å†»ç»“
print("ğŸ§  åˆå§‹åŒ– text_encoder_2 (T5) å¹¶é‡åŒ–å†»ç»“...")
text_encoder_2 = T5EncoderModel.from_pretrained(bfl_repo, subfolder="text_encoder_2", torch_dtype=dtype)
quantize(text_encoder_2, weights=qfloat8)
freeze(text_encoder_2)

# åŠ è½½ vae
print("ğŸ§  åˆå§‹åŒ– vae...")
vae = AutoencoderKL.from_pretrained(bfl_repo, subfolder="vae", torch_dtype=dtype)

# åŠ è½½ transformer å¹¶æ³¨å…¥ LoRA æƒé‡ + é‡åŒ– + å†»ç»“
print("ğŸ§  åˆå§‹åŒ– transformer å¹¶æ³¨å…¥ LoRA æƒé‡...")
transformer = FluxTransformer2DModel.from_pretrained(bfl_repo, subfolder="transformer", torch_dtype=dtype)
state_dict = load_file(f'{lora_model_path}/è‹-FLUXæŠ–éŸ³å°çº¢ä¹¦æè‡´çœŸå®_è‹-FLUXå°çº¢ä¹¦æè‡´çœŸå®V2.safetensors')
transformer.load_state_dict(state_dict, strict=False)
quantize(transformer, weights=qfloat8)
freeze(transformer)

# æ„å»º pipeline
print("ğŸ§  æ„å»º FluxPipeline å¹¶å¯ç”¨è‡ªåŠ¨å¸è½½...")
pipe = FluxPipeline(
    scheduler=scheduler,
    text_encoder=text_encoder,
    tokenizer=tokenizer,
    text_encoder_2=text_encoder_2,
    tokenizer_2=tokenizer_2,
    vae=vae,
    transformer=transformer,
)
pipe.enable_model_cpu_offload()
pipe.enable_vae_slicing()

# åˆ é™¤ä¸å†éœ€è¦çš„ç»„ä»¶ä»¥é‡Šæ”¾å†…å­˜
del text_encoder, text_encoder_2, vae, transformer
torch.cuda.empty_cache()




def translate_text(input_text):
    """ç¿»è¯‘æ–‡æœ¬"""
    try:
    # å¯ä»¥é€‰æ‹©åœ¨è¿™é‡Œæ·»åŠ é‡è¯•é€»è¾‘æˆ–è€…è·³è¿‡å½“å‰ä»»åŠ¡
        result =  translator.translate(input_text)
        return result
    except IndexError as e:
        print(f"Caught an IndexError: {e}")
    return input_text


def generate(prompt, steps, guidance, width, height, seed):
    """ç”Ÿæˆå›¾ç‰‡"""
    torch.cuda.empty_cache()
    print_gpu_memory("ç”Ÿæˆå‰æ˜¾å­˜çŠ¶æ€")
    
    translated_prompt = translate_text(prompt)
    print(f"Prompt: {prompt} â†’ Translated: {translated_prompt}")

    image = pipe(
        prompt=translated_prompt,
        width=width,
        height=height,
        num_inference_steps=steps,
        guidance_scale=guidance,
        generator=torch.Generator(device="cpu").manual_seed(int(seed) if seed != -1 else torch.seed()),
    ).images[0]

    print_gpu_memory("ç”Ÿæˆåæ˜¾å­˜çŠ¶æ€")
    return image


if __name__ == "__main__":
    print("ğŸ“¸ å¼€å§‹æµ‹è¯•å›¾åƒç”Ÿæˆ...")
    prompt = "ä¸€åªå¯çˆ±çš„å°çŒ«"
    img = generate(prompt=prompt, steps=20, guidance=3.5, width=1280, height=720, seed=-1)
    output_path = os.path.join(current_working_directory, "test_output.png")
    img.save(output_path)
    print(f"âœ… å›¾åƒå·²ä¿å­˜è‡³ï¼š{output_path}")